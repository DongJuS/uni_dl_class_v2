{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa8585c8-2260-4b28-9a0b-5f3f79fb24c1",
   "metadata": {},
   "source": [
    "# 1.a_tensor_initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff08b3ef-8ab3-4bb7-af79-652eb104c453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# torch.Tensor class\n",
    "t1 = torch.Tensor([1, 2, 3], device='cpu')\n",
    "print(t1.dtype)   # >>> torch.float32\n",
    "print(t1.device)  # >>> cpu\n",
    "print(t1.requires_grad)  # >>> False\n",
    "print(t1.size())  # torch.Size([3])\n",
    "print(t1.shape)   # torch.Size([3])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64b9cf5b-e90b-4424-aa06-fb6f15052c6b",
   "metadata": {},
   "source": "torch.Tensor는 기본적으로 32-bit 부동 소수점 자료형을 사용하며, 기울기 계산을 추적하지 않습니다 (requires_grad=False). size()와 shape는 텐서의 차원을 반환하며, 결과는 동일합니다"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f680631-c128-4f93-a8fd-4d31d67c0734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## 1\n",
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "t1_cpu = t1.cpu()\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# torch.tensor function\n",
    "t2 = torch.tensor([1, 2, 3], device='cpu')\n",
    "print(t2.dtype)  # >>> torch.int64\n",
    "print(t2.device)  # >>> cpu\n",
    "print(t2.requires_grad)  # >>> False\n",
    "print(t2.size())  # torch.Size([3])\n",
    "print(t2.shape)  # torch.Size([3])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d74894a7-199e-4f9a-8adb-472b7c04f912",
   "metadata": {},
   "source": "torch.tensor() 함수는 입력값의 데이터 타입을 자동으로 추론하여 생성하며, 여기서는 int64 타입을 사용합니다. torch.Tensor와 달리, 데이터 타입을 명시하지 않으면 정수형 데이터를 그대로 유지합니다"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5612bd03-6742-427a-9ae4-ed2262c9ae3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## 2\n",
      "torch.Size([]) 0\n",
      "torch.Size([1]) 1\n",
      "torch.Size([5]) 1\n",
      "torch.Size([5, 1]) 2\n"
     ]
    }
   ],
   "source": [
    "t2_cpu = t2.cpu()\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "a1 = torch.tensor(1)\t\t\t     # shape: torch.Size([]), ndims(=rank): 0\n",
    "print(a1.shape, a1.ndim)\n",
    "\n",
    "a2 = torch.tensor([1])\t\t  \t     # shape: torch.Size([1]), ndims(=rank): 1\n",
    "print(a2.shape, a2.ndim)\n",
    "\n",
    "a3 = torch.tensor([1, 2, 3, 4, 5])   # shape: torch.Size([5]), ndims(=rank): 1\n",
    "print(a3.shape, a3.ndim)\n",
    "\n",
    "a4 = torch.tensor([[1], [2], [3], [4], [5]])   # shape: torch.Size([5, 1]), ndims(=rank): 2\n",
    "print(a4.shape, a4.ndim)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "768edf66-cd9f-4c27-ba1a-b915a6a1207e",
   "metadata": {},
   "source": [
    "텐서의 차원(rank)은 데이터의 배열 구조를 나타냅니다. a1은 스칼라(0차원), a2와 a3는 1차원 벡터, a4는 5x1 행렬로 2차원 배열을 의미합니다. shape와 ndim은 각각 텐서의 크기와 차원을 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b370aec-b9f5-4de1-9c0b-47ec2817ca63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2]) 2\n",
      "torch.Size([3, 2, 1]) 3\n",
      "torch.Size([3, 1, 2, 1]) 4\n",
      "torch.Size([3, 1, 2, 3]) 4\n",
      "torch.Size([3, 1, 2, 3, 1]) 5\n",
      "torch.Size([4, 5]) 2\n",
      "torch.Size([4, 1, 5]) 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 3 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 53\u001B[0m\n\u001B[0;32m     45\u001B[0m a10 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([                 \u001B[38;5;66;03m# shape: torch.Size([4, 1, 5]), ndims(=rank): 3\u001B[39;00m\n\u001B[0;32m     46\u001B[0m     [[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]],\n\u001B[0;32m     47\u001B[0m     [[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]],\n\u001B[0;32m     48\u001B[0m     [[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]],\n\u001B[0;32m     49\u001B[0m     [[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]],\n\u001B[0;32m     50\u001B[0m ])\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28mprint\u001B[39m(a10\u001B[38;5;241m.\u001B[39mshape, a10\u001B[38;5;241m.\u001B[39mndim)\n\u001B[1;32m---> 53\u001B[0m a11 \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43m                 \u001B[49m\u001B[38;5;66;43;03m# ValueError: expected sequence of length 3 at dim 3 (got 2)\u001B[39;49;00m\n\u001B[0;32m     54\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     57\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     58\u001B[0m \u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mValueError\u001B[0m: expected sequence of length 3 at dim 3 (got 2)"
     ]
    }
   ],
   "source": [
    "a5 = torch.tensor([                 # shape: torch.Size([3, 2]), ndims(=rank): 2\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(a5.shape, a5.ndim)\n",
    "\n",
    "a6 = torch.tensor([                 # shape: torch.Size([3, 2, 1]), ndims(=rank): 3\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim)\n",
    "\n",
    "a7 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 1]), ndims(=rank): 4\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)\n",
    "\n",
    "a8 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3]), ndims(=rank): 4\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)\n",
    "\n",
    "\n",
    "a9 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3, 1]), ndims(=rank): 5\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim)\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 5]), ndims(=rank): 2\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 1, 5]), ndims(=rank): 3\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "a11 = torch.tensor([                 # ValueError: expected sequence of length 3 at dim 3 (got 2)\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe489238-056c-40ff-8f50-50eb057db585",
   "metadata": {},
   "source": [
    "텐서는 다양한 차원으로 구성될 수 있으며, 각 차원은 shape와 ndim으로 표현됩니다. a11은 dim=3에서 서로 다른 길이를 가진 리스트를 포함하고 있어 오류가 발생합니다. 텐서의 모든 차원에서 동일한 길이를 유지해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42b4556-abb3-499a-afcd-0eb1b84e669d",
   "metadata": {},
   "source": [
    "# 2.b_tensor_initialization_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d9fcb-5da4-4537-b5fa-61c324cef6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "l1 = [1, 2, 3]\n",
    "t1 = torch.Tensor(l1)\n",
    "\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.tensor(l2)\n",
    "\n",
    "l3 = [1, 2, 3]\n",
    "t3 = torch.as_tensor(l3)\n",
    "\n",
    "l1[0] = 100\n",
    "l2[0] = 100\n",
    "l3[0] = 100\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0586842d-4ae1-4368-8586-4dc7a55d8f09",
   "metadata": {},
   "source": [
    "torch.Tensor와 torch.tensor는 원본 리스트와 독립적인 텐서를 생성하여, 리스트 변경이 텐서에 영향을 미치지 않습니다. 반면 torch.as_tensor는 원본 데이터를 참조하기 때문에 리스트 변경이 t3에 반영됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1d3fd6-0b38-46f0-9276-a8f42013066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l4 = np.array([1, 2, 3])\n",
    "t4 = torch.Tensor(l4)\n",
    "\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.tensor(l5)\n",
    "\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)\n",
    "\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100\n",
    "\n",
    "print(t4)\n",
    "print(t5)\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a0a2965-7ed3-4f23-abf0-a4f6194950aa",
   "metadata": {},
   "source": [
    "torch.Tensor와 torch.tensor는 넘파이 배열의 데이터를 복사하므로, l4와 l5의 변경이 t4와 t5에 영향을 주지 않습니다. 반면 torch.as_tensor는 배열을 참조하기 때문에, l6의 변경이 t6에 반영됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bc7a6d-f7ef-4167-8757-634737fa244d",
   "metadata": {},
   "source": [
    "# 3. c_tensor_initialization_constant_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a056f-3028-4978-a635-88a06a82e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "l4 = np.array([1, 2, 3])\n",
    "t4 = torch.Tensor(l4)\n",
    "\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.tensor(l5)\n",
    "\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)\n",
    "\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100\n",
    "\n",
    "print(t4)\n",
    "print(t5)\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b246cede-769b-4472-ba31-3fbe94c91c4d",
   "metadata": {},
   "source": [
    "torch.Tensor(l4)와 torch.tensor(l5)는 넘파이 배열을 복사하여 생성된 텐서로, 원본 배열 l4, l5의 변경이 t4, t5에 영향을 주지 않습니다. 반면 torch.as_tensor(l6)는 원본 배열 l6를 참조하므로, l6의 변경이 t6에 반영되어 t6의 값도 변경됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb65f5ba-2943-40ce-8241-3054ac7a31bf",
   "metadata": {},
   "source": [
    "# 4. d_tensor_initialization_random_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c492dabb-9371-4e06-a347-1aeb623de14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l4 = np.array([1, 2, 3])\n",
    "t4 = torch.Tensor(l4)\n",
    "\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.tensor(l5)\n",
    "\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)\n",
    "\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100\n",
    "\n",
    "print(t4)\n",
    "print(t5)\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d254a78e-f64c-4092-b2fa-0141fbc8017a",
   "metadata": {},
   "source": [
    "결과를 확인해보면:\n",
    "\n",
    "t4: torch.Tensor(l4)는 넘파이 배열을 복사하므로, l4의 변경이 t4에 반영되지 않아 [1.0, 2.0, 3.0]으로 출력됩니다.\n",
    "t5: torch.tensor(l5)도 배열을 복사하여 생성되므로, l5의 변경이 t5에 반영되지 않아 [1, 2, 3]으로 출력됩니다.\n",
    "t6: torch.as_tensor(l6)는 원본 배열 l6을 참조하므로, l6의 변경이 t6에 반영되어 [100, 2, 3]으로 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed3d2b-6a55-4550-881c-2202f1c3e7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "print()\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1083016-2aa6-4133-9f30-253d540a2be8",
   "metadata": {},
   "source": [
    "torch.rand(2, 3)은 0과 1 사이의 무작위 값을 가진 텐서를 생성합니다. torch.manual_seed(1729)을 설정하면 난수 생성기가 고정되어, 이후 생성되는 random3과 random4는 동일한 값을 가지게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf41fca-2c3c-49a6-b001-07a37128ed36",
   "metadata": {},
   "source": [
    "# 5. e_tensor_type_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d55f5a-c7c1-4bb5-9478-2753c3e47c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "print()\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c48bf352-77b7-4b71-a33e-e1aa3e4dd88c",
   "metadata": {},
   "source": [
    "random1과 random2는 각각 독립적인 무작위 값으로 생성됩니다. torch.manual_seed(1729)을 설정한 후의 random3과 random4는 같은 시드로 인해 동일한 값을 가지게 됩니다. 이는 재현성을 위해 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6678e57-8f57-4c1e-8e06-b8389ebfffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_d = torch.ones(10, 2, dtype=torch.double)\n",
    "short_e = torch.tensor([[1, 2]], dtype=torch.short)\n",
    "\n",
    "double_d = torch.zeros(10, 2).double()\n",
    "short_e = torch.ones(10, 2).short()\n",
    "\n",
    "double_d = torch.zeros(10, 2).to(torch.double)\n",
    "short_e = torch.ones(10, 2).to(dtype=torch.short)\n",
    "\n",
    "double_d = torch.zeros(10, 2).type(torch.double)\n",
    "short_e = torch.ones(10, 2). type(dtype=torch.short)\n",
    "\n",
    "print(double_d.dtype)\n",
    "print(short_e.dtype)\n",
    "\n",
    "double_f = torch.rand(5, dtype=torch.double)\n",
    "short_g = double_f.to(torch.short)\n",
    "print((double_f * short_g).dtype)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ff7aa23-eb02-4ba6-b007-1b20d2b7663f",
   "metadata": {},
   "source": [
    "double_d와 short_e는 각각 torch.double과 torch.short 타입으로 설정됩니다. double_f는 double 타입, short_g는 short 타입으로 변환되며, 이들의 곱셈 결과는 torch.double 타입이 됩니다. 텐서의 곱셈은 더 높은 정밀도의 데이터 타입을 유지합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5259d0eb-ab92-4d92-8978-3a429bba4992",
   "metadata": {},
   "source": [
    "# 6. f_tensor_operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fe1a01-e52d-4f94-9e7b-d5201dc2367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(size=(2, 3))\n",
    "t2 = torch.ones(size=(2, 3))\n",
    "t3 = torch.add(t1, t2)\n",
    "t4 = t1 + t2\n",
    "print(t3)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b07add4-3ddd-4362-b149-8faf94f8e6a2",
   "metadata": {},
   "source": [
    "t3와 t4는 모두 t1과 t2의 원소별 합으로, 동일한 결과를 출력합니다. torch.add(t1, t2)는 명시적인 덧셈 함수 사용이고, t1 + t2는 연산자 오버로딩을 통한 간편한 덧셈 표현입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1586b87-4136-4bb6-b143-d7c1e6894a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5 = torch.sub(t1, t2)\n",
    "t6 = t1 - t2\n",
    "print(t5)\n",
    "print(t6)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c16ffc57-2d5d-4aa3-adb2-4673bd73a9bc",
   "metadata": {},
   "source": [
    "t5와 t6는 모두 t1과 t2의 원소별 차로, 동일한 결과를 출력합니다. torch.sub(t1, t2)는 명시적인 뺄셈 함수 사용이고, t1 - t2는 연산자 오버로딩을 통한 간단한 뺄셈 표현입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa23b074-5262-4903-a79c-c061b77d5c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "t7 = torch.mul(t1, t2)\n",
    "t8 = t1 * t2\n",
    "print(t7)\n",
    "print(t8)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82e0983d-f367-4003-81ad-8ce8b2bf5997",
   "metadata": {},
   "source": [
    "t7와 t8은 모두 t1과 t2의 원소별 곱셈 결과로, 동일하게 출력됩니다. torch.mul(t1, t2)는 명시적인 곱셈 함수 사용이고, t1 * t2는 연산자 오버로딩을 통해 더 간단하게 표현됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ea8f01-8fde-41bb-8cb0-5041793c77c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t9 = torch.div(t1, t2)\n",
    "t10 = t1 / t2\n",
    "print(t9)\n",
    "print(t10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b69b6d2-40ce-484a-b32f-acec5369d125",
   "metadata": {},
   "source": [
    "torch.div()와 / 연산자는 PyTorch에서 동일하게 작동하여 두 텐서의 원소별 나눗셈을 수행합니다.\n",
    "torch.div()와 / 연산자를 사용하여 나누기를 할 수 있으며, 원하는 스타일에 맞게 선택하여 사용하면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee874d6f-2f3a-460f-aba3-4bd2d5a6360f",
   "metadata": {},
   "source": [
    "# 7. g_tensor_operations_mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7f6001-10a6-4a10-af28-7ceaa382b869",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.dot(\n",
    "  torch.tensor([2, 3]), torch.tensor([2, 1])\n",
    ")\n",
    "print(t1, t1.size())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ba1c6fc-64d5-435e-8e7d-df96de803777",
   "metadata": {},
   "source": [
    "torch.dot() 함수는 두 1차원 텐서(벡터) 간의 내적(dot product)을 계산합니다. 결과는 스칼라 값으로 반환되며, 스칼라에는 size() 메서드가 없기 때문에 오류가 발생할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f4c318-3428-44c2-ac4b-ed669add8609",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.randn(2, 3)\n",
    "t3 = torch.randn(3, 2)\n",
    "t4 = torch.mm(t2, t3)\n",
    "print(t4, t4.size())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53d62eca-874f-422a-96c5-a8aa608c7b64",
   "metadata": {},
   "source": [
    "torch.mm() 함수는 두 2차원 행렬 간의 행렬 곱(matrix multiplication)을 수행합니다. t2와 t3의 크기에 맞게 행렬 곱을 수행하여 결과를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d72a8-129d-4978-b2a8-e35ed7eb50a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(10, 4, 5)\n",
    "t7 = torch.bmm(t5, t6)\n",
    "print(t7.size())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "504ebd94-87f0-4197-97c3-daa976f15981",
   "metadata": {},
   "source": [
    "torch.bmm() 함수는 배치 행렬 곱(batch matrix multiplication)을 수행합니다. 즉, 3차원 텐서(배치 크기를 포함한 3차원 행렬) 간의 행렬 곱셈을 수행할 때 사용됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b55b14-9359-4b16-afed-5b534992cb90",
   "metadata": {},
   "source": [
    "# 8. h_tensor_operations_matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a2ebb8-a0c4-4b98-a543-a15f91e3cc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# vector x vector: dot product\n",
    "t1 = torch.randn(3)\n",
    "t2 = torch.randn(3)\n",
    "print(torch.matmul(t1, t2).size())  # torch.Size([])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83ca4066-e1bb-4a78-a0d4-b97ee02461be",
   "metadata": {},
   "source": [
    "torch.matmul()을 벡터 간의 연산에 사용하면 내적(dot product)이 수행됩니다. 이 경우 결과는 스칼라 값이 되며, 스칼라 값의 크기는 torch.Size([])로 표시됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd9e8f3-b0bd-4fc4-969e-3e332c6aea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix x vector: broadcasted dot\n",
    "t3 = torch.randn(3, 4)\n",
    "t4 = torch.randn(4)\n",
    "print(torch.matmul(t3, t4).size())  # torch.Size([3])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9b6b8e2-306a-48e5-9bf4-195a7a0e249a",
   "metadata": {},
   "source": [
    "torch.matmul()을 사용하여 행렬과 벡터를 곱할 때, 벡터는 행렬의 마지막 차원과 곱해지고 결과는 벡터가 됩니다. 이를 \"broadcasted dot\"라고 부르기도 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01a6d5c-c061-454c-ada5-29e1dc5eed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batched matrix x vector: broadcasted dot\n",
    "t5 = torch.randn(10, 3, 4) # 4개의 원소를 가진 벡터\n",
    "t6 = torch.randn(4) # 배치된 행렬과 벡터의 곱셈 (broadcasted dot product)\n",
    "print(torch.matmul(t5, t6).size())  # torch.Size([10, 3])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b16b31a8-43bf-49ec-964f-6574397736e2",
   "metadata": {},
   "source": [
    "torch.matmul()을 사용하여 배치된 행렬(batched matrix)과 벡터를 곱할 때, 벡터는 각 배치의 행렬에 대해 곱셈이 수행되며 결과는 배치된 벡터가 됩니다. 이 과정에서 벡터는 브로드캐스트 되어 각 배치에 대해 계산됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67daba59-86e1-4233-83a7-fa780bfe9958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batched matrix x batched matrix: bmm\n",
    "t7 = torch.randn(10, 3, 4)\n",
    "t8 = torch.randn(10, 4, 5)\n",
    "print(torch.matmul(t7, t8).size())  # torch.Size([10, 3, 5])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15f9d7fd-b332-41cb-9d84-6a212599bce0",
   "metadata": {},
   "source": [
    "torch.matmul()을 사용하여 배치된 행렬 간의 곱셈을 수행할 때, 각 배치에 대해 행렬 곱셈이 수행됩니다. torch.bmm()와 동일하게 동작하지만, torch.bmm()는 두 입력 텐서가 (B, N, M)과 (B, M, P) 크기를 가져야 하는 반면, torch.matmul()은 더 유연한 입력 크기를 지원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e91e6a46-a7f2-4c24-b809-45c63be09343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# batched matrix x matrix: bmm\n",
    "t9 = torch.randn(10, 3, 4)\n",
    "t10 = torch.randn(4, 5)\n",
    "print(torch.matmul(t9, t10).size())  # torch.Size([10, 3, 5])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a65c5885-aff0-411e-9cf2-a302112c05aa",
   "metadata": {},
   "source": [
    "주어진 코드에서 torch.matmul()을 사용하여 배치된 행렬과 일반 행렬 간의 곱셈을 수행하면 각 배치에 대해 행렬 곱셈이 브로드캐스트 방식으로 적용됩니다. 그러나 torch.bmm()는 두 텐서 모두가 배치 행렬이어야만 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b35d90c-c4ca-4189-a282-89db6add3aba",
   "metadata": {},
   "source": [
    "# 9. i_tensor_broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "520cec17-1fcf-4e17-b98c-5ad2d88825a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n",
      "################################################## 1\n",
      "tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n",
      "################################################## 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1차원 텐서 t1 정의\n",
    "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# 스칼라 값 t2 정의\n",
    "t2 = 2.0\n",
    "\n",
    "# t1과 t2의 원소별 곱셈 (broadcasting)\n",
    "print(t1 * t2)  # tensor([2.0, 4.0, 6.0])\n",
    "\n",
    "print(\"#\" * 50, 1)  # 구분선 출력\n",
    "\n",
    "# 3x2 행렬 t3 정의\n",
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
    "\n",
    "# 1차원 텐서 t4 정의\n",
    "t4 = torch.tensor([4, 5])\n",
    "\n",
    "# t3와 t4의 원소별 뺄셈 (broadcasting)\n",
    "print(t3 - t4)  # tensor([[-4, -4], [-2, -1], [ 6,  5]])\n",
    "\n",
    "print(\"#\" * 50, 2)  # 구분선 출력"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d241eb6-65e6-46f7-bbc6-4cfdb88e284c",
   "metadata": {},
   "source": [
    "t5 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(t5 + 2.0)  # t5.add(2.0)\n",
    "print(t5 - 2.0)  # t5.sub(2.0)\n",
    "print(t5 * 2.0)  # t5.mul(2.0)\n",
    "print(t5 / 2.0)  # t5.div(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c72c763-4f36-4988-a7bb-f289d58eaac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n",
      "################################################## 4\n",
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "tensor([[ 8,  9],\n",
      "        [ 7, 10]])\n",
      "################################################## 5\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "  return x / 255\n",
    "\n",
    "\n",
    "t6 = torch.randn(3, 28, 28)\n",
    "print(normalize(t6).size())\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "t7 = torch.tensor([[1, 2], [0, 3]])  # torch.Size([2, 2])\n",
    "t8 = torch.tensor([[3, 1]])  # torch.Size([1, 2])\n",
    "t9 = torch.tensor([[5], [2]])  # torch.Size([2, 1])\n",
    "t10 = torch.tensor([7])  # torch.Size([1])\n",
    "print(t7 + t8)   # >>> tensor([[4, 3], [3, 4]])\n",
    "print(t7 + t9)   # >>> tensor([[6, 7], [2, 5]])\n",
    "print(t8 + t9)   # >>> tensor([[8, 6], [5, 3]])\n",
    "print(t7 + t10)  # >>> tensor([[ 8, 9], [ 7, 10]])\n",
    "\n",
    "print(\"#\" * 50, 5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ad0563e-d08d-4446-bc4c-9849dbc13aab",
   "metadata": {},
   "source": [
    "핵심 코드:\n",
    "정규화 함수:\n",
    "\n",
    "return x / 255\n",
    "입력 텐서 x의 각 원소를 255로 나누어 0에서 1 사이의 값으로 정규화합니다.\n",
    "\n",
    "브로드캐스팅 덧셈:\n",
    "\n",
    "print(t7 + t8)  # tensor([[4, 3], [3, 4]])\n",
    "t7(2x2 텐서)와 t8(1x2 텐서)을 브로드캐스팅하여 더합니다. t8이 t7의 각 행에 반복적으로 더해집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c205147e-7442-49df-af06-a61c80cea887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([5, 3, 4, 1])\n",
      "################################################## 6\n"
     ]
    }
   ],
   "source": [
    "t11 = torch.ones(4, 3, 2)\n",
    "t12 = t11 * torch.rand(3, 2)  # 3rd & 2nd dims identical to t11, dim 0 absent\n",
    "print(t12.shape)\n",
    "\n",
    "t13 = torch.ones(4, 3, 2)\n",
    "t14 = t13 * torch.rand(3, 1)  # 3rd dim = 1, 2nd dim is identical to t13\n",
    "print(t14.shape)\n",
    "\n",
    "t15 = torch.ones(4, 3, 2)\n",
    "t16 = t15 * torch.rand(1, 2)  # 3rd dim is identical to t15, 2nd dim is 1\n",
    "print(t16.shape)\n",
    "\n",
    "t17 = torch.ones(5, 3, 4, 1)\n",
    "t18 = torch.rand(3, 1, 1)  # 2nd dim is identical to t17, 3rd and 4th dims are 1\n",
    "print((t17 + t18).size())\n",
    "\n",
    "print(\"#\" * 50, 6)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df340a6a-b800-4621-b305-45ac76308adf",
   "metadata": {},
   "source": [
    "핵심 코드:\n",
    "브로드캐스팅 곱셈:\n",
    "\n",
    "t12 = t11 * torch.rand(3, 2)\n",
    "t11(4, 3, 2) 텐서에 torch.rand(3, 2)를 곱할 때, torch.rand(3, 2)는 t11의 첫 번째 차원에 브로드캐스팅됩니다. 결과 크기: (4, 3, 2).\n",
    "\n",
    "브로드캐스팅 덧셈:\n",
    "\n",
    "print((t17 + t18).size())  # torch.Size([5, 3, 4, 1])\n",
    "t17(5, 3, 4, 1)과 t18(3, 1, 1)을 더할 때, t18의 두 번째, 세 번째, 네 번째 차원이 t17과 동일한 차원으로 브로드캐스팅됩니다. 결과 크기: (5, 3, 4, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c636f28a-f19e-44c3-be8a-7cc302d59807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n",
      "torch.Size([3, 3, 3])\n",
      "################################################## 7\n",
      "tensor([5., 5., 5., 5.])\n",
      "tensor([25., 25., 25., 25.])\n",
      "tensor([  1.,   4.,  27., 256.])\n"
     ]
    }
   ],
   "source": [
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(3, 1, 1)\n",
    "print((t19 + t20).size())  # torch.Size([5, 3, 4, 1])\n",
    "\n",
    "t21 = torch.empty(1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())  # torch.Size([3, 1, 7])\n",
    "\n",
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())  # torch.Size([3, 3, 3])\n",
    "\n",
    "# t25 = torch.empty(5, 2, 4, 1)\n",
    "# t26 = torch.empty(3, 1, 1)\n",
    "# print((t25 + t26).size())\n",
    "# RuntimeError: The size of tensor a (2) must match\n",
    "# the size of tensor b (3) at non-singleton dimension 1\n",
    "\n",
    "print(\"#\" * 50, 7)\n",
    "\n",
    "t27 = torch.ones(4) * 5\n",
    "print(t27)  # >>> tensor([ 5, 5, 5, 5])\n",
    "\n",
    "t28 = torch.pow(t27, 2)\n",
    "print(t28)  # >>> tensor([ 25, 25, 25, 25])\n",
    "\n",
    "exp = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "a = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "t29 = torch.pow(a, exp)\n",
    "print(t29)  # >>> tensor([   1.,    4.,   27.,  256.])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34102869-4450-4f9a-915b-a50fb005c7e5",
   "metadata": {},
   "source": [
    "핵심 코드:\n",
    "브로드캐스팅 덧셈:\n",
    "\n",
    "print((t19 + t20).size())  # torch.Size([5, 3, 4, 1])\n",
    "t19(5, 1, 4, 1)과 t20(3, 1, 1)을 더할 때, t20의 첫 번째 차원이 t19의 첫 번째 차원에, 나머지 차원이 동일하게 브로드캐스팅됩니다. 결과 크기: (5, 3, 4, 1).\n",
    "\n",
    "지수 계산:\n",
    "\n",
    "t29 = torch.pow(a, exp)\n",
    "a(1, 2, 3, 4)와 exp(1, 2, 3, 4)의 각각 원소를 제곱하여 결과: [1^1, 2^2, 3^3, 4^4] = [1, 4, 27, 256]을 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aabb3b-2ebe-42d3-a9f5-38f14ddbeb27",
   "metadata": {},
   "source": [
    "# 10. j_tensor_indexing_slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3758a22-ccf3-41f5-9b81-64712b222264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([ 4,  9, 14])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9],\n",
    "   [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "print(x[1])  # >>> tensor([5, 6, 7, 8, 9])\n",
    "print(x[:, 1])  # >>> tensor([1, 6, 11])\n",
    "print(x[1, 2])  # >>> tensor(7)\n",
    "print(x[:, -1])  # >>> tensor([4, 9, 14)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88a0a355-de30-498e-84cc-f1a6e51169a1",
   "metadata": {},
   "source": [
    "핵심 코드:\n",
    "행 인덱싱:\n",
    "\n",
    "print(x[1])  # tensor([5, 6, 7, 8, 9])\n",
    "첫 번째 행(index 1)을 선택하여 [5, 6, 7, 8, 9]을 출력합니다.\n",
    "\n",
    "열 인덱싱:\n",
    "\n",
    "print(x[:, 1])  # tensor([1, 6, 11])\n",
    "모든 행(:)에서 두 번째 열(index 1)의 값을 선택하여 [1, 6, 11]을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd818150-44fd-4c0a-b2eb-166a4b1fcf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n",
      "################################################## 2\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "################################################## 3\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n",
      "tensor([[3, 4],\n",
      "        [6, 7]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "source": [
    "print(x[1:])  # >>> tensor([[ 5,  6,  7,  8,  9], [10, 11, 12, 13, 14]])\n",
    "print(x[1:, 3:])  # >>> tensor([[ 8,  9], [13, 14]])\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "y = torch.zeros((6, 6))\n",
    "y[1:4, 2] = 1\n",
    "print(y)\n",
    "\n",
    "print(y[1:4, 1:4])\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "z = torch.tensor(\n",
    "  [[1, 2, 3, 4],\n",
    "   [2, 3, 4, 5],\n",
    "   [5, 6, 7, 8]]\n",
    ")\n",
    "print(z[:2])\n",
    "print(z[1:, 1:3])\n",
    "print(z[:, 1:])\n",
    "\n",
    "z[1:, 1:3] = 0\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9359039-92a4-43a4-b643-b1d34fdae259",
   "metadata": {},
   "source": [
    "핵심 코드:\n",
    "슬라이싱 인덱싱:\n",
    "\n",
    "print(x[1:, 3:])  # tensor([[ 8,  9], [13, 14]])\n",
    "첫 번째 행부터 끝까지 (1:), 세 번째 열부터 끝까지 (3:)의 서브 텐서를 선택합니다. 결과는 [[8, 9], [13, 14]]입니다.\n",
    "\n",
    "텐서 값 업데이트:\n",
    "\n",
    "y[1:4, 2] = 1\n",
    "텐서 y에서 두 번째 열(index 2)의 두 번째 행부터 네 번째 행까지(1:4) 값을 1로 업데이트합니다. 해당 열의 값이 [1, 1, 1]로 변경됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af58cc6-8abd-4ab1-b363-506b8e68efc2",
   "metadata": {},
   "source": [
    "# 11. k_tensor_reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48933bcd-ae92-4463-949b-a684c4658494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = t1.view(3, 2)  # Shape becomes (3, 2)\n",
    "t3 = t1.reshape(1, 6)  # Shape becomes (1, 6)\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.arange(8).view(2, 4)  # Shape becomes (2, 4)\n",
    "t5 = torch.arange(6).view(2, 3)  # Shape becomes (2, 3)\n",
    "print(t4)\n",
    "print(t5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc996c61-1dde-4ed9-917f-411bb03c61a0",
   "metadata": {},
   "source": [
    "핵심 코드:\n",
    "텐서 모양 변경:\n",
    "\n",
    "t2 = t1.view(3, 2)\n",
    "t1(2, 3)을 (3, 2)로 재구조화하여 [[1, 2], [3, 4], [5, 6]] 형태로 변환합니다.\n",
    "\n",
    "텐서 생성 및 변형:\n",
    "\n",
    "t4 = torch.arange(8).view(2, 4)\n",
    "torch.arange(8)로 [0, 1, 2, 3, 4, 5, 6, 7] 텐서를 생성하고 (2, 4) 모양으로 변형하여 [[0, 1, 2, 3], [4, 5, 6, 7]]로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecbcadc9-b7ed-4b69-b05d-e2b689a3c60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "################################################## 2\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n",
      "################################################## 3\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "################################################## 4\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (1, 3, 1)\n",
    "t6 = torch.tensor([[[1], [2], [3]]])\n",
    "\n",
    "# Remove all dimensions of size 1\n",
    "t7 = t6.squeeze()  # Shape becomes (3,)\n",
    "\n",
    "# Remove dimension at position 0\n",
    "t8 = t6.squeeze(0)  # Shape becomes (3, 1)\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "# Original tensor with shape (3,)\n",
    "t9 = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Add a new dimension at position 1\n",
    "t10 = t9.unsqueeze(1)  # Shape becomes (3, 1)\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.tensor(\n",
    "  [[1, 2, 3],\n",
    "   [4, 5, 6]]\n",
    ")\n",
    "t12 = t11.unsqueeze(1)  # Shape becomes (2, 1, 3)\n",
    "print(t12, t12.shape)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "# Original tensor with shape (2, 3)\n",
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Flatten the tensor\n",
    "t14 = t13.flatten()  # Shape becomes (6,)\n",
    "\n",
    "print(t14)\n",
    "\n",
    "# Original tensor with shape (2, 2, 2)\n",
    "t15 = torch.tensor([[[1, 2],\n",
    "                     [3, 4]],\n",
    "                    [[5, 6],\n",
    "                     [7, 8]]])\n",
    "t16 = torch.flatten(t15)\n",
    "\n",
    "t17 = torch.flatten(t15, start_dim=1)\n",
    "\n",
    "print(t16)\n",
    "print(t17)\n",
    "\n",
    "print(\"#\" * 50, 4)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18aa06e9-5446-450f-96e2-2dadd0a65fa4",
   "metadata": {},
   "source": [
    "차원 제거:\n",
    "\n",
    "t7 = t6.squeeze()  # Shape becomes (3,)\n",
    "텐서 t6에서 크기가 1인 모든 차원을 제거하여 (3,) 모양으로 변환합니다. [[1], [2], [3]]에서 [1, 2, 3]이 됩니다.\n",
    "\n",
    "텐서 펼치기:\n",
    "\n",
    "t14 = t13.flatten()  # Shape becomes (6,)\n",
    "t13(2, 3)을 (6,)으로 평탄화하여 [1, 2, 3, 4, 5, 6] 형태로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1b99230-a5b1-41d5-9198-3c05fe69870a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t18 = torch.randn(2, 3, 5)\n",
    "print(t18.shape)  # >>> torch.Size([2, 3, 5])\n",
    "print(torch.permute(t18, (2, 0, 1)).size())  # >>> torch.Size([5, 2, 3])\n",
    "\n",
    "# Original tensor with shape (2, 3)\n",
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Permute the dimensions\n",
    "t20 = torch.permute(t19, dims=(0, 1))  # Shape becomes (2, 3) still\n",
    "t21 = torch.permute(t19, dims=(1, 0))  # Shape becomes (3, 2)\n",
    "print(t20)\n",
    "print(t21)\n",
    "\n",
    "# Transpose the tensor\n",
    "t22 = torch.transpose(t19, 0, 1)  # Shape becomes (3, 2)\n",
    "\n",
    "print(t22)\n",
    "\n",
    "t23 = torch.t(t19)  # Shape becomes (3, 2)\n",
    "\n",
    "print(t23)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a5ca32d-487e-47ce-b716-6f27367585e7",
   "metadata": {},
   "source": [
    "핵심 코드:\n",
    "차원 재배치:\n",
    "\n",
    "torch.permute(t18, (2, 0, 1))  # Shape becomes (5, 2, 3)\n",
    "t18(2, 3, 5)의 차원을 (2, 0, 1) 순서로 재배치하여 (5, 2, 3) 모양으로 변경합니다.\n",
    "\n",
    "텐서 전치:\n",
    "\n",
    "t22 = torch.transpose(t19, 0, 1)  # Shape becomes (3, 2)\n",
    "t19(2, 3)을 전치(transpose)하여 (3, 2)로 변환합니다. t19의 행과 열을 교환하여 [1, 4], [2, 5], [3, 6] 형태가 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd8f5c9-5491-47d1-b5fd-39205d713928",
   "metadata": {},
   "source": [
    "# 12 l_tensor_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "033c60e0-7587-4c26-a568-a4817591dd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.zeros([2, 1, 3])\n",
    "t2 = torch.zeros([2, 3, 3])\n",
    "t3 = torch.zeros([2, 2, 3])\n",
    "\n",
    "t4 = torch.cat([t1, t2, t3], dim=1)\n",
    "print(t4.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ecb3b8ad-4f5e-4244-be71-9a1a80ce6bf2",
   "metadata": {},
   "source": [
    "핵심 코드:\n",
    "\n",
    "t4 = torch.cat([t1, t2, t3], dim=1)\n",
    "세 개의 텐서 t1(2, 1, 3), t2(2, 3, 3), t3(2, 2, 3)을 두 번째 차원(dim=1)을 따라 연결하여 (2, 6, 3) 모양의 텐서 t4를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c2d352f-bb8e-4519-8926-7ffd82eedf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t6 = torch.arange(3, 8)  # tensor([3, 4, 5, 6, 7])\n",
    "\n",
    "t7 = torch.cat((t5, t6), dim=0)\n",
    "print(t7.shape)  # >>> torch.Size([8])\n",
    "print(t7)  # >>> tensor([0, 1, 2, 3, 4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d3e835f9-4c82-4c6c-8c61-0579ad07dc18",
   "metadata": {},
   "source": [
    "핵심 코드:\n",
    "\n",
    "t7 = torch.cat((t5, t6), dim=0)\n",
    "1차원 텐서 t5(0, 1, 2)와 t6(3, 4, 5, 6, 7)을 첫 번째 차원(dim=0)에서 연결하여 (8,) 모양의 텐서 [0, 1, 2, 3, 4, 5, 6, 7]을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "282a2f26-ea2f-43bd-bb1d-4e39d46c1132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n",
      "################################################## 3\n",
      "torch.Size([6, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]])\n",
      "torch.Size([2, 9])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
      "################################################## 4\n"
     ]
    }
   ],
   "source": [
    "t8 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t9 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "# 2차원 텐서간 병합\n",
    "t10 = torch.cat((t8, t9), dim=0)\n",
    "print(t10.size())  # >>> torch.Size([4, 3])\n",
    "print(t10)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11]])\n",
    "\n",
    "t11 = torch.cat((t8, t9), dim=1)\n",
    "print(t11.size())  # >>>torch.Size([2, 6])\n",
    "print(t11)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8],\n",
    "#             [ 3,  4,  5,  9, 10, 11]])\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "t12 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t13 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t14 = torch.arange(12, 18).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "t15 = torch.cat((t12, t13, t14), dim=0)\n",
    "print(t15.size())  # >>> torch.Size([6, 3])\n",
    "print(t15)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11],\n",
    "#             [12, 13, 14],\n",
    "#             [15, 16, 17]])\n",
    "\n",
    "t16 = torch.cat((t12, t13, t14), dim=1)\n",
    "print(t16.size())  # >>> torch.Size([2, 9])\n",
    "print(t16)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
    "#             [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
    "\n",
    "print(\"#\" * 50, 4)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "821841df-eaa7-460f-af46-0802e8576a3a",
   "metadata": {},
   "source": [
    "핵심 코드:\n",
    "행 병합:\n",
    "\n",
    "t10 = torch.cat((t8, t9), dim=0)  # Shape becomes (4, 3)\n",
    "t8(2, 3)과 t9(2, 3)을 행 기준(dim=0)으로 병합하여 (4, 3) 모양의 텐서를 생성합니다.\n",
    "\n",
    "열 병합:\n",
    "\n",
    "t16 = torch.cat((t12, t13, t14), dim=1)  # Shape becomes (2, 9)\n",
    "t12, t13, t14를 열 기준(dim=1)으로 병합하여 (2, 9) 모양의 텐서를 생성합니다. 각 행의 값들이 수평으로 연결됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca6ac6b6-8db4-4ae2-bcbc-6089dd9ccc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
      "         [ 3,  4,  5,  9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "\n",
    "t19 = torch.cat((t17, t18), dim=0)\n",
    "print(t19.size())  # >>> torch.Size([2, 2, 3])\n",
    "print(t19)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5]],\n",
    "#             [[ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t20 = torch.cat((t17, t18), dim=1)\n",
    "print(t20.size())  # >>> torch.Size([1, 4, 3])\n",
    "print(t20)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5],\n",
    "#              [ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t21 = torch.cat((t17, t18), dim=2)\n",
    "print(t21.size())  # >>> torch.Size([1, 2, 6])\n",
    "print(t21)\n",
    "# >>> tensor([[[ 0,  1,  2,  6,  7,  8],\n",
    "#              [ 3,  4,  5,  9, 10, 11]]])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c99e554-e238-41f1-94cf-a016367b3f80",
   "metadata": {},
   "source": [
    "핵심 코드:\n",
    "0차원 병합 (배치 기준):\n",
    "\n",
    "t19 = torch.cat((t17, t18), dim=0)  # Shape becomes (2, 2, 3)\n",
    "t17(1, 2, 3)과 t18(1, 2, 3)을 첫 번째 차원(배치, dim=0)에서 병합하여 (2, 2, 3) 모양의 텐서를 생성합니다.\n",
    "\n",
    "2차원 병합 (세로 기준):\n",
    "\n",
    "t21 = torch.cat((t17, t18), dim=2)  # Shape becomes (1, 2, 6)\n",
    "t17과 t18을 세 번째 차원(dim=2)에서 병합하여 (1, 2, 6) 모양의 텐서를 생성합니다. 각 행의 요소들이 수평으로 연결됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f270c1a-4662-4c92-a859-329a12df4320",
   "metadata": {},
   "source": [
    "# 13. m_tensor_stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7645f2d6-8bf9-4761-950c-dc4013ec6b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n",
      "################################################## 1\n",
      "torch.Size([3]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "t3 = torch.stack([t1, t2], dim=0)\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)\n",
    "print(t3.shape, t3.equal(t4))\n",
    "\n",
    "t5 = torch.stack([t1, t2], dim=1)\n",
    "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)\n",
    "print(t5.shape, t5.equal(t6))\n",
    "\n",
    "t7 = torch.stack([t1, t2], dim=2)\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2)\n",
    "print(t7.shape, t7.equal(t8))\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t9 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t10 = torch.arange(3, 6)  # tensor([3, 4, 5])\n",
    "\n",
    "print(t9.size(), t10.size())\n",
    "# >>> torch.Size([3]) torch.Size([3])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7983c8b-7500-4775-9f25-6e2b771d783a",
   "metadata": {},
   "source": [
    "핵심 코드:\n",
    "텐서 쌓기 (stack):\n",
    "\n",
    "t3 = torch.stack([t1, t2], dim=0)\n",
    "t1과 t2를 첫 번째 차원(dim=0)에 쌓아 (2, 2, 3) 모양의 텐서를 생성합니다. stack은 새로운 차원을 추가하면서 텐서들을 쌓습니다.\n",
    "\n",
    "차원 추가 후 병합 (cat):\n",
    "\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)\n",
    "t1과 t2에 차원을 추가 (unsqueeze(dim=0))하고, 첫 번째 차원(dim=0)에서 병합하여 (2, 2, 3) 모양의 텐서를 생성합니다. stack과 unsqueeze + cat의 결과가 동일합니다.\n",
    "\n",
    "추가 설명:\n",
    "stack은 새로운 차원을 추가하며 쌓고, cat은 기존 차원에 맞게 텐서를 병합합니다. unsqueeze와 함께 사용하면 동일한 효과를 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "465f491a-f27c-480c-af05-725efa942920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "True\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "t11 = torch.stack((t9, t10), dim=0)\n",
    "print(t11.size())  # >>> torch.Size([2,3])\n",
    "print(t11)\n",
    "# >>> tensor([[0, 1, 2],\n",
    "#             [3, 4, 5]])\n",
    "\n",
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)\n",
    "print(t11.equal(t12))\n",
    "# >>> True\n",
    "\n",
    "t13 = torch.stack((t9, t10), dim=1)\n",
    "print(t13.size())  # >>> torch.Size([3,2])\n",
    "print(t13)\n",
    "# >>> tensor([[0, 3],\n",
    "#             [1, 4],\n",
    "#             [2, 5]])\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
    "print(t13.equal(t14))\n",
    "# >>> True"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a38b4f7-d5a3-4b72-97f9-919297b57155",
   "metadata": {},
   "source": [
    "핵심 코드:\n",
    "1차원 텐서 쌓기 (stack):\n",
    "\n",
    "t13 = torch.stack((t9, t10), dim=1)\n",
    "t9과 t10을 두 번째 차원(dim=1)에 쌓아 (3, 2) 모양의 텐서를 생성합니다. 결과는 각 요소를 세로로 정렬하여 [[0, 3], [1, 4], [2, 5]]입니다.\n",
    "\n",
    "차원 추가 후 병합 (cat):\n",
    "\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
    "t9과 t10에 차원을 추가 (unsqueeze(dim=1))한 후, 두 번째 차원(dim=1)에서 병합하여 (3, 2) 모양의 텐서를 생성합니다. stack과 unsqueeze + cat의 결과가 동일합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617dc2bc-28bd-4928-a972-3b356ee06f55",
   "metadata": {},
   "source": [
    "# 14. n_tensor_vstack_hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3334dd95-4c99-4f7e-bdb9-a53e1483d381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "t3 = torch.vstack((t1, t2))\n",
    "print(t3)\n",
    "# >>> tensor([[1, 2, 3],\n",
    "#             [4, 5, 6]])\n",
    "\n",
    "t4 = torch.tensor([[1], [2], [3]])\n",
    "t5 = torch.tensor([[4], [5], [6]])\n",
    "t6 = torch.vstack((t4, t5))\n",
    "# >>> tensor([[1],\n",
    "#             [2],\n",
    "#             [3],\n",
    "#             [4],\n",
    "#             [5],\n",
    "#             [6]])\n",
    "\n",
    "t7 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t7.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89202250-0123-4c4a-899e-847bb20c4c48",
   "metadata": {},
   "source": [
    "핵심 코드:\n",
    "수직 스택 (vstack):\n",
    "\n",
    "t3 = torch.vstack((t1, t2))\n",
    "1차원 텐서 t1과 t2를 수직으로 쌓아 [1, 2, 3]과 [4, 5, 6]을 행으로 가지는 (2, 3) 모양의 2차원 텐서를 생성합니다.\n",
    "\n",
    "3차원 텐서 모양:\n",
    "\n",
    "print(t7.shape)  # torch.Size([2, 2, 3])\n",
    "t7은 2개의 [2, 3] 모양의 행렬이 포함된 3차원 텐서입니다. 첫 번째 차원은 배치, 두 번째 차원은 행(row), 세 번째 차원은 열(column)을 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2c3e403-3f74-4b41-8eab-746a8dd93c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "torch.Size([4, 2, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[19, 20, 21],\n",
      "         [22, 23, 24]]])\n",
      "################################################## 1\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "t8 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t8.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t9 = torch.vstack([t7, t8])\n",
    "print(t9.shape)\n",
    "# >>> (4, 2, 3)\n",
    "\n",
    "print(t9)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12]],\n",
    "#             [[13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[19, 20, 21],\n",
    "#              [22, 23, 24]]])\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t10 = torch.tensor([1, 2, 3])\n",
    "t11 = torch.tensor([4, 5, 6])\n",
    "t12 = torch.hstack((t10, t11))\n",
    "print(t12)\n",
    "# >>> tensor([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "t13 = torch.tensor([[1], [2], [3]])\n",
    "t14 = torch.tensor([[4], [5], [6]])\n",
    "t15 = torch.hstack((t13, t14))\n",
    "print(t15)\n",
    "# >>> tensor([[1, 4],\n",
    "#             [2, 5],\n",
    "#             [3, 6]])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8f58acd-91c5-421e-87c3-8bbea7b1a058",
   "metadata": {},
   "source": [
    "핵심 코드:\n",
    "3차원 텐서의 수직 스택 (vstack):\n",
    "\n",
    "t9 = torch.vstack([t7, t8])\n",
    "3차원 텐서 t7과 t8을 첫 번째 차원(배치 차원)을 기준으로 수직으로 쌓아 (4, 2, 3) 모양의 텐서를 생성합니다. 이로 인해 두 텐서가 합쳐지면서 배치 크기가 4로 증가합니다.\n",
    "\n",
    "수평 스택 (hstack):\n",
    "\n",
    "t12 = torch.hstack((t10, t11))\n",
    "1차원 텐서 t10과 t11을 수평으로 쌓아 [1, 2, 3, 4, 5, 6]으로 병합하여 1차원 (6,) 모양의 텐서를 생성합니다.\n",
    "\n",
    "t15 = torch.hstack((t13, t14))\n",
    "2차원 텐서 t13과 t14를 수평으로 쌓아 (3, 2) 모양의 텐서를 생성합니다. 각 열이 결합되어 [1, 4], [2, 5], [3, 6]으로 병합됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3644198-3834-425e-bee0-8c1ce4e83695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "t16 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t16.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t17 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t17.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t18 = torch.hstack([t16, t17])\n",
    "print(t18.shape)\n",
    "# >>> (2, 4, 3)\n",
    "\n",
    "print(t18)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6],\n",
    "#              [13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12],\n",
    "#              [19, 20, 21],\n",
    "#              [22, 23, 24]]])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7b99ff2-8296-4575-b25c-ad2b418784a5",
   "metadata": {},
   "source": [
    "핵심 코드:\n",
    "3차원 텐서의 수평 스택 (hstack):\n",
    "\n",
    "t18 = torch.hstack([t16, t17])\n",
    "3차원 텐서 t16과 t17을 두 번째 차원(dim=1)에서 수평으로 쌓아 (2, 4, 3) 모양의 텐서를 생성합니다. 각 배치에서 행(row)이 결합되어 [ [1, 2, 3], [4, 5, 6], [13, 14, 15], [16, 17, 18] ]와 같이 행이 추가됩니다.\n",
    "\n",
    "기존 t16과 t17은 (2, 2, 3) 크기였으나, 수평으로 결합되면서 행이 합쳐져 (2, 4, 3)이 됩니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
